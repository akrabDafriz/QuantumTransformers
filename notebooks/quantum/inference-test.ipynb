{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63217a6d",
   "metadata": {},
   "source": [
    "# Testing inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8c98517",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-01 14:11:45.285221: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-11-01 14:11:45.285357: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-11-01 14:11:45.365667: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-11-01 14:12:13.570056: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Please first ``pip install -U qiskit`` to enable related functionality in translation module\n",
      "Please first ``pip install -U cirq`` to enable related functionality in translation module\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import pickle\n",
    "import jax.numpy as jnp\n",
    "from flax import serialization\n",
    "\n",
    "# Import custom modules\n",
    "from quantum_transformers.datasets import get_mlm_dataloaders\n",
    "from quantum_transformers.training import train_and_evaluate\n",
    "from quantum_transformers.transformers import Transformer\n",
    "from quantum_transformers.quantum_layer import get_circuit\n",
    "from quantum_transformers.inference import save_model, load_model, predict_masked_token, evaluate_on_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dd49e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Setting up environment ---\n",
      "Available JAX devices:\n",
      "- gpu:0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU)\n"
     ]
    }
   ],
   "source": [
    "# 1. SETUP \n",
    "print(\"Setting up environment...\")\n",
    "\n",
    "# Ensure TF does not see GPU and grab all GPU memory.\n",
    "tf.config.set_visible_devices([], device_type='GPU')\n",
    "\n",
    "# Define directories\n",
    "data_dir = './data'\n",
    "CLASSICAL_MODEL_PATH = './models/mlm_classical'\n",
    "QUANTUM_MODEL_PATH = './models/mlm_quantum'\n",
    "os.makedirs(CLASSICAL_MODEL_PATH, exist_ok=True)\n",
    "os.makedirs(QUANTUM_MODEL_PATH, exist_ok=True)\n",
    "\n",
    "# Print JAX devices\n",
    "print(\"Available JAX devices:\")\n",
    "for d in jax.devices():\n",
    "    print(f\"- {d} ({d.device_kind})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34155bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 2. Loading and preparing dataset ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc6891430a32417b8b7ac3da97b249a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/75710 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (874 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0b02a852aba462c985f883269a833f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8413 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a276118a251f463bb072c5ac5df0c014",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9347 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89565273b7734c8faeaf31acd47573ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/75710 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4f5d242c7804e8e941f266bcab0879b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8413 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1e0fa375447441eb81d0e7dac78e2c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9347 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset loading complete.\n",
      "Tokenizer vocabulary size: 30522\n",
      "Initialization batch shape: (16, 128)\n"
     ]
    }
   ],
   "source": [
    "#2. LOAD DATA \n",
    "print(\"\\nLoading and preparing dataset...\")\n",
    "\n",
    "# Set data loading parameters\n",
    "block_size = 128  # The size of our text chunks\n",
    "batch_size = 16   # How many chunks to process at once\n",
    "\n",
    "# Get the dataloaders and the tokenizer\n",
    "(train_dataloader_gen, val_dataloader_gen, test_dataloader_gen), tokenizer = get_mlm_dataloaders(\n",
    "    dataset_name='Helsinki-NLP/opus_books',\n",
    "    model_checkpoint='bert-base-uncased',\n",
    "    block_size=block_size,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset loading complete.\")\n",
    "print(f\"Tokenizer vocabulary size: {len(tokenizer.vocab)}\")\n",
    "\n",
    "# Get one batch for model initialization\n",
    "try:\n",
    "    init_batch_tuple = next(iter(train_dataloader_gen()))\n",
    "    init_batch_input = init_batch_tuple[0]\n",
    "    print(f\"Initialization batch shape: {init_batch_input.shape}\")\n",
    "except StopIteration:\n",
    "    print(\"Error: Training dataloader is empty. Cannot initialize models.\")\n",
    "    # In a notebook, you might want to raise an error or just stop\n",
    "    # return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d252b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 3. Starting Classical Transformer Training ---\n"
     ]
    }
   ],
   "source": [
    "#3. TRAIN CLASSICAL MODEL \n",
    "print(\"\\nStarting Classical Transformer Training\")\n",
    "\n",
    "classical_model = Transformer(\n",
    "    num_tokens=len(tokenizer.vocab),\n",
    "    max_seq_len=block_size,\n",
    "    task='mlm',\n",
    "    hidden_size=8,\n",
    "    num_heads=2,\n",
    "    num_transformer_blocks=4,\n",
    "    mlp_hidden_size=4,\n",
    "    dropout=0.1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9858fdd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 3. Starting Classical Transformer Training ---\n",
      "Number of parameters = 521498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 1189it [00:59, 19.96it/s, Loss=10.3391, PPL=30917.71]\n",
      "                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 10.5682, Val Loss = 10.3013, Val PPL = 29772.06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2: 1189it [00:58, 20.32it/s, Loss=9.8803, PPL=19542.40] \n",
      "                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss = 10.1327, Val Loss = 9.9135, Val PPL = 20201.25\n",
      "Total training time = 115.72s, best validation loss = 9.9135 at epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 150it [00:03, 38.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss = 9.9059, Test PPL = 20047.90\n",
      "\n",
      "--- Classical Transformer Training Finished ---\n",
      "Final Test Perplexity: 20047.9043\n",
      "Model and tokenizer saved to ./models/mlm_classical\n"
     ]
    }
   ],
   "source": [
    "(classical_test_loss, classical_test_ppl), classical_best_state = train_and_evaluate(\n",
    "    model=classical_model,\n",
    "    train_dataloader=train_dataloader_gen,\n",
    "    val_dataloader=val_dataloader_gen,\n",
    "    test_dataloader=test_dataloader_gen,\n",
    "    task='mlm',\n",
    "    num_epochs=2  # A shorter run for demonstration\n",
    ")\n",
    "\n",
    "print(\"\\n--- Classical Transformer Training Finished ---\")\n",
    "print(f\"Final Test Perplexity: {classical_test_ppl:.4f}\")\n",
    "\n",
    "# Save the classical model\n",
    "save_model(classical_best_state, tokenizer, CLASSICAL_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e841af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 4. Starting Quantum Transformer Training ---\n"
     ]
    }
   ],
   "source": [
    "#4. TRAIN QUANTUM MODEL \n",
    "print(\"\\nStarting Quantum Transformer Training\")\n",
    "\n",
    "quantum_model = Transformer(\n",
    "    num_tokens=len(tokenizer.vocab),\n",
    "    max_seq_len=block_size,\n",
    "    task='mlm',\n",
    "    hidden_size=8,\n",
    "    num_heads=2,\n",
    "    num_transformer_blocks=4,\n",
    "    mlp_hidden_size=4,\n",
    "    dropout=0.1,\n",
    "    quantum_attn_circuit=get_circuit(),  # Activate the quantum attention\n",
    "    quantum_mlp_circuit=get_circuit()    # Activate the quantum MLP\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71039870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 4. Starting Quantum Transformer Training ---\n",
      "Number of parameters = 520490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 1189it [09:24,  2.10it/s, Loss=10.4944, PPL=36111.71]\n",
      "                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 10.5890, Val Loss = 10.3688, Val PPL = 31849.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2: 1189it [06:24,  3.09it/s, Loss=10.1799, PPL=26367.33]\n",
      "                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss = 10.2563, Val Loss = 10.0297, Val PPL = 22691.01\n",
      "Total training time = 1010.13s, best validation loss = 10.0297 at epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 150it [00:34,  4.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss = 10.0258, Test PPL = 22603.11\n",
      "\n",
      "--- Quantum Transformer Training Finished ---\n",
      "Final Test Perplexity: 22603.1074\n",
      "Model and tokenizer saved to ./models/mlm_quantum\n"
     ]
    }
   ],
   "source": [
    "(quantum_test_loss, quantum_test_ppl), quantum_best_state = train_and_evaluate(\n",
    "    model=quantum_model,\n",
    "    train_dataloader=train_dataloader_gen,\n",
    "    val_dataloader=val_dataloader_gen,\n",
    "    test_dataloader=test_dataloader_gen,\n",
    "    task='mlm',\n",
    "    num_epochs=2  # A shorter run for demonstration\n",
    ")\n",
    "\n",
    "print(\"\\n--- Quantum Transformer Training Finished ---\")\n",
    "print(f\"Final Test Perplexity: {quantum_test_ppl:.4f}\")\n",
    "\n",
    "# Save the quantum model\n",
    "save_model(quantum_best_state, tokenizer, QUANTUM_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefebb02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "--- 5. Running Inference ---\n",
      "==============================\n",
      "\n",
      "--- Loading Classical Model for Inference ---\n",
      "Model and tokenizer loaded from ./models/mlm_classical\n",
      "\n",
      "--- Loading Quantum Model for Inference ---\n",
      "Model and tokenizer loaded from ./models/mlm_quantum\n",
      "\n",
      "==============================\n",
      "--- Classical Model Batch Prediction ---\n",
      "--- Running batch inference on 5 sentences ---\n",
      "\n",
      "Example 1:\n",
      "Input: 'He went to the [MASK] to buy some bread.'\n",
      "Top predictions:\n",
      "  - authentication  (Logit: 3.32)\n",
      "  - ##gar           (Logit: 3.27)\n",
      "  - rec             (Logit: 3.19)\n",
      "\n",
      "Example 2:\n",
      "Input: 'The capital of France is [MASK].'\n",
      "Top predictions:\n",
      "  - ##dah           (Logit: 3.44)\n",
      "  - listened        (Logit: 3.33)\n",
      "  - mermaid         (Logit: 3.16)\n",
      "\n",
      "Example 3:\n",
      "Input: 'She put the book on the [MASK].'\n",
      "Top predictions:\n",
      "  - gland           (Logit: 3.35)\n",
      "  - ##thic          (Logit: 3.22)\n",
      "  - ##dah           (Logit: 3.14)\n",
      "\n",
      "Example 4:\n",
      "Input: 'Let's go for a [MASK] in the park.'\n",
      "Top predictions:\n",
      "  - gland           (Logit: 3.37)\n",
      "  - ##thic          (Logit: 3.20)\n",
      "  - ##par           (Logit: 3.17)\n",
      "\n",
      "Example 5:\n",
      "Input: 'The [MASK] is barking at the mailman.'\n",
      "Top predictions:\n",
      "  - talbot          (Logit: 3.52)\n",
      "  - ##thic          (Logit: 3.44)\n",
      "  - listened        (Logit: 3.42)\n",
      "\n",
      "--- Batch inference complete ---\n",
      "\n",
      "==============================\n",
      "--- Quantum Model Batch Prediction ---\n",
      "--- Running batch inference on 5 sentences ---\n",
      "\n",
      "Example 1:\n",
      "Input: 'He went to the [MASK] to buy some bread.'\n",
      "Top predictions:\n",
      "  - munitions       (Logit: 2.97)\n",
      "  - lost            (Logit: 2.93)\n",
      "  - remember        (Logit: 2.93)\n",
      "\n",
      "Example 2:\n",
      "Input: 'The capital of France is [MASK].'\n",
      "Top predictions:\n",
      "  - of              (Logit: 3.30)\n",
      "  - mermaid         (Logit: 3.26)\n",
      "  - wa              (Logit: 3.10)\n",
      "\n",
      "Example 3:\n",
      "Input: 'She put the book on the [MASK].'\n",
      "Top predictions:\n",
      "  - mermaid         (Logit: 3.05)\n",
      "  - of              (Logit: 3.01)\n",
      "  - munitions       (Logit: 3.00)\n",
      "\n",
      "Example 4:\n",
      "Input: 'Let's go for a [MASK] in the park.'\n",
      "Top predictions:\n",
      "  - mermaid         (Logit: 3.05)\n",
      "  - of              (Logit: 3.01)\n",
      "  - ##pace          (Logit: 3.00)\n",
      "\n",
      "Example 5:\n",
      "Input: 'The [MASK] is barking at the mailman.'\n",
      "Top predictions:\n",
      "  - mermaid         (Logit: 3.20)\n",
      "  - listened        (Logit: 3.08)\n",
      "  - of              (Logit: 2.99)\n",
      "\n",
      "--- Batch inference complete ---\n",
      "\n",
      "--- Experiment Complete ---\n"
     ]
    }
   ],
   "source": [
    "#5. RUN INFERENCE \n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\"=== Running Inference ===\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "print(\"\\n--- Loading Classical Model for Inference ---\")\n",
    "classical_params, classical_tokenizer = load_model(\n",
    "    model_path=CLASSICAL_MODEL_PATH,\n",
    "    model_instance=classical_model,\n",
    "    init_batch=init_batch_input\n",
    ")\n",
    "\n",
    "print(\"\\n--- Loading Quantum Model for Inference ---\")\n",
    "quantum_params, quantum_tokenizer = load_model(\n",
    "    model_path=QUANTUM_MODEL_PATH,\n",
    "    model_instance=quantum_model,\n",
    "    init_batch=init_batch_input\n",
    ")\n",
    "\n",
    "# --- NEW: Short test dataset ---\n",
    "inference_dataset = [\n",
    "    \"He went to the [MASK] to buy some bread.\",\n",
    "    \"The capital of France is [MASK].\",\n",
    "    \"She put the book on the [MASK].\",\n",
    "    \"Let's go for a [MASK] in the park.\",\n",
    "    \"The [MASK] is barking at the mailman.\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\"--- Classical Model Batch Prediction ---\")\n",
    "evaluate_on_list(\n",
    "    texts=inference_dataset, \n",
    "    model=classical_model, \n",
    "    params=classical_params, \n",
    "    tokenizer=classical_tokenizer,\n",
    "    top_k=3  # Show top 3 predictions\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\"--- Quantum Model Batch Prediction ---\")\n",
    "evaluate_on_list(\n",
    "    texts=inference_dataset, \n",
    "    model=quantum_model, \n",
    "    params=quantum_params, \n",
    "    tokenizer=quantum_tokenizer,\n",
    "    top_k=3  # Show top 3 predictions\n",
    ")\n",
    "\n",
    "print(\"\\n--- Experiment Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a779baac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
