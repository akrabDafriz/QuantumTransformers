{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63217a6d",
   "metadata": {},
   "source": [
    "# Testing inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8c98517",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-01 21:56:29.544426: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1762008989.555151 2087615 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1762008989.558590 2087615 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1762008989.568420 2087615 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1762008989.568429 2087615 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1762008989.568430 2087615 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1762008989.568431 2087615 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "Please first ``pip install -U qiskit`` to enable related functionality in translation module\n",
      "Please first ``pip install -U cirq`` to enable related functionality in translation module\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import pickle\n",
    "import jax.numpy as jnp\n",
    "from flax import serialization\n",
    "\n",
    "# Import custom modules\n",
    "from quantum_transformers.datasets import get_mlm_dataloaders\n",
    "from quantum_transformers.training import train_and_evaluate\n",
    "from quantum_transformers.transformers import Transformer\n",
    "from quantum_transformers.quantum_layer import get_circuit\n",
    "from quantum_transformers.inference import save_model, load_model, predict_masked_token, evaluate_on_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38dd49e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up environment...\n",
      "Available JAX devices:\n",
      "- gpu:0 (NVIDIA GeForce RTX 4090)\n"
     ]
    }
   ],
   "source": [
    "# 1. SETUP \n",
    "print(\"Setting up environment...\")\n",
    "\n",
    "# Ensure TF does not see GPU and grab all GPU memory.\n",
    "tf.config.set_visible_devices([], device_type='GPU')\n",
    "\n",
    "# Define directories\n",
    "data_dir = './data'\n",
    "CLASSICAL_MODEL_PATH = './models/mlm_classical'\n",
    "QUANTUM_MODEL_PATH = './models/mlm_quantum'\n",
    "os.makedirs(CLASSICAL_MODEL_PATH, exist_ok=True)\n",
    "os.makedirs(QUANTUM_MODEL_PATH, exist_ok=True)\n",
    "\n",
    "# Print JAX devices\n",
    "print(\"Available JAX devices:\")\n",
    "for d in jax.devices():\n",
    "    print(f\"- {d} ({d.device_kind})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34155bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading and preparing dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd492e0335c44d39951306fa901020e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/75710 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (580 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d02cae01aa6459d88bd0a8ea4aa6c3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8413 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42f10ca626f5425ebd8bfebeee4e4f8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9347 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f09302cb73949a4924be6b79690acf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/75710 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da2d5a1094d34f8f9b475d94f301f63e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8413 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9860d75cef43444c9ffebf35e86154eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9347 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset loading complete.\n",
      "Tokenizer vocabulary size: 30522\n",
      "Initialization batch shape: (16, 128)\n"
     ]
    }
   ],
   "source": [
    "#2. LOAD DATA \n",
    "print(\"\\nLoading and preparing dataset...\")\n",
    "\n",
    "# Set data loading parameters\n",
    "block_size = 128  # The size of our text chunks\n",
    "batch_size = 16   # How many chunks to process at once\n",
    "\n",
    "# Get the dataloaders and the tokenizer\n",
    "(train_dataloader_gen, val_dataloader_gen, test_dataloader_gen), tokenizer = get_mlm_dataloaders(\n",
    "    dataset_name='Helsinki-NLP/opus_books',\n",
    "    model_checkpoint='bert-base-uncased',\n",
    "    block_size=block_size,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset loading complete.\")\n",
    "print(f\"Tokenizer vocabulary size: {len(tokenizer.vocab)}\")\n",
    "\n",
    "# Get one batch for model initialization\n",
    "try:\n",
    "    init_batch_tuple = next(iter(train_dataloader_gen()))\n",
    "    init_batch_input = init_batch_tuple[0]\n",
    "    print(f\"Initialization batch shape: {init_batch_input.shape}\")\n",
    "except StopIteration:\n",
    "    print(\"Error: Training dataloader is empty. Cannot initialize models.\")\n",
    "    # In a notebook, you might want to raise an error or just stop\n",
    "    # return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d252b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Classical Transformer Training\n"
     ]
    }
   ],
   "source": [
    "#3. TRAIN CLASSICAL MODEL \n",
    "print(\"\\nStarting Classical Transformer Training\")\n",
    "\n",
    "classical_model = Transformer(\n",
    "    num_tokens=len(tokenizer.vocab),\n",
    "    max_seq_len=block_size,\n",
    "    task='mlm',\n",
    "    hidden_size=8,\n",
    "    num_heads=2,\n",
    "    num_transformer_blocks=4,\n",
    "    mlp_hidden_size=4,\n",
    "    dropout=0.1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9858fdd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters = 521498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 1192it [00:17, 68.62it/s, Loss=10.4501, PPL=34547.27] \n",
      "                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 10.5670, Val Loss = 10.2990, Val PPL = 29702.53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 1192it [00:11, 103.14it/s, Loss=9.8817, PPL=19568.73] \n",
      "                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss = 10.1359, Val Loss = 9.9129, Val PPL = 20189.85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 1192it [00:11, 101.39it/s, Loss=9.5651, PPL=14258.14]\n",
      "                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss = 9.7721, Val Loss = 9.5709, Val PPL = 14341.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 1192it [00:11, 100.28it/s, Loss=9.3605, PPL=11620.23]\n",
      "                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss = 9.4268, Val Loss = 9.2310, Val PPL = 10208.59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 1192it [00:12, 98.98it/s, Loss=8.8217, PPL=6780.01] \n",
      "                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss = 9.0626, Val Loss = 8.8602, Val PPL = 7045.69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 1192it [00:12, 96.66it/s, Loss=8.2523, PPL=3836.51]\n",
      "                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss = 8.6824, Val Loss = 8.4643, Val PPL = 4742.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 1192it [00:12, 95.95it/s, Loss=8.0463, PPL=3122.25]\n",
      "                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss = 8.3153, Val Loss = 8.1079, Val PPL = 3320.69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 1192it [00:12, 94.92it/s, Loss=7.7956, PPL=2429.83]\n",
      "                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss = 7.9525, Val Loss = 7.7378, Val PPL = 2293.34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 1192it [00:12, 93.05it/s, Loss=7.5697, PPL=1938.57]\n",
      "                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss = 7.6459, Val Loss = 7.4699, Val PPL = 1754.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 1192it [00:12, 97.25it/s, Loss=7.2749, PPL=1443.56]\n",
      "                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss = 7.3824, Val Loss = 7.2455, Val PPL = 1401.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 1192it [00:12, 98.04it/s, Loss=6.9125, PPL=1004.75]\n",
      "                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train Loss = 7.1713, Val Loss = 7.0709, Val PPL = 1177.19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 1192it [00:12, 96.62it/s, Loss=7.2897, PPL=1465.14]\n",
      "                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Train Loss = 7.0278, Val Loss = 6.9523, Val PPL = 1045.53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 1192it [00:12, 96.66it/s, Loss=6.7979, PPL=895.98] \n",
      "                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Train Loss = 6.9326, Val Loss = 6.8951, Val PPL = 987.47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 1192it [00:12, 97.19it/s, Loss=6.6591, PPL=779.84] \n",
      "                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Train Loss = 6.8748, Val Loss = 6.8371, Val PPL = 931.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 1192it [00:12, 97.89it/s, Loss=6.8049, PPL=902.21] \n",
      "                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Train Loss = 6.8133, Val Loss = 6.7908, Val PPL = 889.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 1192it [00:12, 97.63it/s, Loss=6.4027, PPL=603.46] \n",
      "                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: Train Loss = 6.7814, Val Loss = 6.7666, Val PPL = 868.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 1192it [00:12, 98.25it/s, Loss=6.8051, PPL=902.43]  \n",
      "                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: Train Loss = 6.7534, Val Loss = 6.7339, Val PPL = 840.42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: 1192it [00:12, 98.52it/s, Loss=6.4398, PPL=626.25] \n",
      "                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: Train Loss = 6.7163, Val Loss = 6.7051, Val PPL = 816.59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: 1192it [00:11, 99.40it/s, Loss=6.6392, PPL=764.46]  \n",
      "                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: Train Loss = 6.7127, Val Loss = 6.6586, Val PPL = 779.47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 1192it [00:11, 99.80it/s, Loss=6.3387, PPL=566.04]  \n",
      "                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: Train Loss = 6.6977, Val Loss = 6.6971, Val PPL = 810.07\n",
      "Total training time = 264.35s, best validation loss = 6.6586 at epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 147it [00:01, 104.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss = 6.6782, Test PPL = 794.87\n",
      "\n",
      "--- Classical Transformer Training Finished ---\n",
      "Final Test Perplexity: 794.8747\n",
      "Model and tokenizer saved to ./models/mlm_classical\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "(classical_test_loss, classical_test_ppl), classical_best_state = train_and_evaluate(\n",
    "    model=classical_model,\n",
    "    train_dataloader=train_dataloader_gen,\n",
    "    val_dataloader=val_dataloader_gen,\n",
    "    test_dataloader=test_dataloader_gen,\n",
    "    task='mlm',\n",
    "    num_epochs=20  # Change this for different training epoch numbers\n",
    ")\n",
    "\n",
    "print(\"\\n--- Classical Transformer Training Finished ---\")\n",
    "print(f\"Final Test Perplexity: {classical_test_ppl:.4f}\")\n",
    "\n",
    "# Save the classical model\n",
    "save_model(classical_best_state, tokenizer, CLASSICAL_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0e841af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Quantum Transformer Training\n"
     ]
    }
   ],
   "source": [
    "#4. TRAIN QUANTUM MODEL \n",
    "print(\"\\nStarting Quantum Transformer Training\")\n",
    "\n",
    "quantum_model = Transformer(\n",
    "    num_tokens=len(tokenizer.vocab),\n",
    "    max_seq_len=block_size,\n",
    "    task='mlm',\n",
    "    hidden_size=8,\n",
    "    num_heads=2,\n",
    "    num_transformer_blocks=4,\n",
    "    mlp_hidden_size=4,\n",
    "    dropout=0.1,\n",
    "    quantum_attn_circuit=get_circuit(),  # Activate the quantum attention\n",
    "    quantum_mlp_circuit=get_circuit()    # Activate the quantum MLP\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71039870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 4. Starting Quantum Transformer Training ---\n",
      "Number of parameters = 520490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 1189it [09:24,  2.10it/s, Loss=10.4944, PPL=36111.71]\n",
      "                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 10.5890, Val Loss = 10.3688, Val PPL = 31849.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2: 1189it [06:24,  3.09it/s, Loss=10.1799, PPL=26367.33]\n",
      "                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss = 10.2563, Val Loss = 10.0297, Val PPL = 22691.01\n",
      "Total training time = 1010.13s, best validation loss = 10.0297 at epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 150it [00:34,  4.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss = 10.0258, Test PPL = 22603.11\n",
      "\n",
      "--- Quantum Transformer Training Finished ---\n",
      "Final Test Perplexity: 22603.1074\n",
      "Model and tokenizer saved to ./models/mlm_quantum\n"
     ]
    }
   ],
   "source": [
    "(quantum_test_loss, quantum_test_ppl), quantum_best_state = train_and_evaluate(\n",
    "    model=quantum_model,\n",
    "    train_dataloader=train_dataloader_gen,\n",
    "    val_dataloader=val_dataloader_gen,\n",
    "    test_dataloader=test_dataloader_gen,\n",
    "    task='mlm',\n",
    "    num_epochs=20  # Change this for different training epoch numbers\n",
    ")\n",
    "\n",
    "print(\"\\n--- Quantum Transformer Training Finished ---\")\n",
    "print(f\"Final Test Perplexity: {quantum_test_ppl:.4f}\")\n",
    "\n",
    "# Save the quantum model\n",
    "save_model(quantum_best_state, tokenizer, QUANTUM_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aefebb02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "=== Running Inference ===\n",
      "==============================\n",
      "\n",
      "--- Loading Classical Model for Inference ---\n",
      "Model and tokenizer loaded from ./models/mlm_classical\n",
      "\n",
      "--- Loading Quantum Model for Inference ---\n",
      "Model and tokenizer loaded from ./models/mlm_quantum\n",
      "\n",
      "==============================\n",
      "--- Classical Model Batch Prediction ---\n",
      "--- Running batch inference on 5 sentences ---\n",
      "\n",
      "Example 1:\n",
      "Input: 'He went to the [MASK] to buy some bread.'\n",
      "Top predictions:\n",
      "  - ,               (Logit: 7.08)\n",
      "  - the             (Logit: 6.65)\n",
      "  - \"               (Logit: 6.18)\n",
      "\n",
      "Example 2:\n",
      "Input: 'The capital of France is [MASK].'\n",
      "Top predictions:\n",
      "  - ,               (Logit: 7.14)\n",
      "  - the             (Logit: 6.57)\n",
      "  - to              (Logit: 6.21)\n",
      "\n",
      "Example 3:\n",
      "Input: 'She put the book on the [MASK].'\n",
      "Top predictions:\n",
      "  - ,               (Logit: 6.95)\n",
      "  - the             (Logit: 6.77)\n",
      "  - \"               (Logit: 6.26)\n",
      "\n",
      "Example 4:\n",
      "Input: 'Let's go for a [MASK] in the park.'\n",
      "Top predictions:\n",
      "  - ,               (Logit: 6.96)\n",
      "  - the             (Logit: 6.77)\n",
      "  - \"               (Logit: 6.24)\n",
      "\n",
      "Example 5:\n",
      "Input: 'The [MASK] is barking at the mailman.'\n",
      "Top predictions:\n",
      "  - ,               (Logit: 6.99)\n",
      "  - the             (Logit: 6.65)\n",
      "  - .               (Logit: 6.16)\n",
      "\n",
      "--- Batch inference complete ---\n",
      "\n",
      "==============================\n",
      "--- Quantum Model Batch Prediction ---\n",
      "--- Running batch inference on 5 sentences ---\n",
      "\n",
      "Example 1:\n",
      "Input: 'He went to the [MASK] to buy some bread.'\n",
      "Top predictions:\n",
      "  - munitions       (Logit: 2.97)\n",
      "  - lost            (Logit: 2.93)\n",
      "  - remember        (Logit: 2.93)\n",
      "\n",
      "Example 2:\n",
      "Input: 'The capital of France is [MASK].'\n",
      "Top predictions:\n",
      "  - of              (Logit: 3.30)\n",
      "  - mermaid         (Logit: 3.26)\n",
      "  - wa              (Logit: 3.10)\n",
      "\n",
      "Example 3:\n",
      "Input: 'She put the book on the [MASK].'\n",
      "Top predictions:\n",
      "  - mermaid         (Logit: 3.05)\n",
      "  - of              (Logit: 3.01)\n",
      "  - munitions       (Logit: 3.00)\n",
      "\n",
      "Example 4:\n",
      "Input: 'Let's go for a [MASK] in the park.'\n",
      "Top predictions:\n",
      "  - mermaid         (Logit: 3.05)\n",
      "  - of              (Logit: 3.01)\n",
      "  - ##pace          (Logit: 3.00)\n",
      "\n",
      "Example 5:\n",
      "Input: 'The [MASK] is barking at the mailman.'\n",
      "Top predictions:\n",
      "  - mermaid         (Logit: 3.20)\n",
      "  - listened        (Logit: 3.08)\n",
      "  - of              (Logit: 2.99)\n",
      "\n",
      "--- Batch inference complete ---\n",
      "\n",
      "--- Experiment Complete ---\n"
     ]
    }
   ],
   "source": [
    "#5. RUN INFERENCE \n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\"=== Running Inference ===\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "print(\"\\n--- Loading Classical Model for Inference ---\")\n",
    "classical_params, classical_tokenizer = load_model(\n",
    "    model_path=CLASSICAL_MODEL_PATH,\n",
    "    model_instance=classical_model,\n",
    "    init_batch=init_batch_input\n",
    ")\n",
    "\n",
    "print(\"\\n--- Loading Quantum Model for Inference ---\")\n",
    "quantum_params, quantum_tokenizer = load_model(\n",
    "    model_path=QUANTUM_MODEL_PATH,\n",
    "    model_instance=quantum_model,\n",
    "    init_batch=init_batch_input\n",
    ")\n",
    "\n",
    "# --- NEW: Short test dataset ---\n",
    "inference_dataset = [\n",
    "    \"He went to the [MASK] to buy some bread.\",\n",
    "    \"The capital of France is [MASK].\",\n",
    "    \"She put the book on the [MASK].\",\n",
    "    \"Let's go for a [MASK] in the park.\",\n",
    "    \"The [MASK] is barking at the mailman.\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\"--- Classical Model Batch Prediction ---\")\n",
    "evaluate_on_list(\n",
    "    texts=inference_dataset, \n",
    "    model=classical_model, \n",
    "    params=classical_params, \n",
    "    tokenizer=classical_tokenizer,\n",
    "    top_k=3  # Show top 3 predictions\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\"--- Quantum Model Batch Prediction ---\")\n",
    "evaluate_on_list(\n",
    "    texts=inference_dataset, \n",
    "    model=quantum_model, \n",
    "    params=quantum_params, \n",
    "    tokenizer=quantum_tokenizer,\n",
    "    top_k=3  # Show top 3 predictions\n",
    ")\n",
    "\n",
    "print(\"\\n--- Experiment Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a779baac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
