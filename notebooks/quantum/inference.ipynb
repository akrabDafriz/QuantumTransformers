{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0422aff2-e099-4bd4-b3e2-9efbed8f83c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-07 15:43:51.809389: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1762505031.821205 3591764 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1762505031.824933 3591764 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1762505031.837425 3591764 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1762505031.837432 3591764 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1762505031.837433 3591764 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1762505031.837434 3591764 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-11-07 15:43:51.840245: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Please first ``pip install -U qiskit`` to enable related functionality in translation module\n",
      "Please first ``pip install -U cirq`` to enable related functionality in translation module\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available JAX devices:\n",
      "- gpu:0 (NVIDIA GeForce RTX 4090)\n"
     ]
    }
   ],
   "source": [
    "# --- 1. FRAMEWORK SETUP (MUST BE FIRST) ---\n",
    "import tensorflow as tf\n",
    "# Ensure TF does not see GPU and grab all GPU memory.\n",
    "# This MUST run before JAX is imported.\n",
    "tf.config.set_visible_devices([], device_type='GPU')\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import os\n",
    "from flax.training import train_state\n",
    "from transformers import AutoTokenizer\n",
    "import pickle\n",
    "\n",
    "# --- END FRAMEWORK SETUP ---\n",
    "\n",
    "# Import custom modules\n",
    "# We need to add the project root to the path if running from 'notebooks/quantum'\n",
    "import sys\n",
    "sys.path.append('../..') \n",
    "\n",
    "# --- CORRECTED IMPORT: This is the function we will use ---\n",
    "from quantum_transformers.inference import load_model \n",
    "from quantum_transformers.transformers import Transformer\n",
    "from quantum_transformers.quantum_layer import get_circuit\n",
    "\n",
    "print(\"Available JAX devices:\")\n",
    "for d in jax.devices():\n",
    "    print(f\"- {d} ({d.device_kind})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ea22fca-0aec-4efd-869a-2d588800a25f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classical model path: /dafriz/QuantumTransformers/models/mlm_classical\n",
      "Quantum model path: /dafriz/QuantumTransformers/models/mlm_quantum\n"
     ]
    }
   ],
   "source": [
    "# --- 2. DEFINE PATHS ---\n",
    "# These paths must match the ones used in mlm_training.py\n",
    "\n",
    "CLASSICAL_MODEL_PATH = '../../models/mlm_classical'\n",
    "QUANTUM_MODEL_PATH = '../../models/mlm_quantum'\n",
    "\n",
    "print(f\"Classical model path: {os.path.abspath(CLASSICAL_MODEL_PATH)}\")\n",
    "print(f\"Quantum model path: {os.path.abspath(QUANTUM_MODEL_PATH)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92f13698-2c9a-4bd4-bd57-513a47975a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading classical model...\n",
      "Model and tokenizer loaded from ../../models/mlm_classical\n",
      "Classical tokenizer vocabulary size: 1000\n",
      "\n",
      "Loading quantum model...\n",
      "Model and tokenizer loaded from ../../models/mlm_quantum\n",
      "Quantum tokenizer vocabulary size: 1000\n"
     ]
    }
   ],
   "source": [
    "# --- 3. LOAD MODELS AND TOKENIZERS (CORRECTED) ---\n",
    "\n",
    "# Define model hyperparameters (must match training)\n",
    "VOCAB_SIZE = 1000\n",
    "MAX_SEQ_LEN = 128\n",
    "HIDDEN_SIZE = 8\n",
    "NUM_HEADS = 2\n",
    "NUM_BLOCKS = 4\n",
    "MLP_HIDDEN_SIZE = 8\n",
    "\n",
    "# Create a dummy batch to initialize the model state (required by load_model)\n",
    "# Shape is (batch_size, max_seq_len)\n",
    "init_batch = jnp.ones((1, MAX_SEQ_LEN), dtype=jnp.int32)\n",
    "\n",
    "# --- Load Classical Model ---\n",
    "print(\"Loading classical model...\")\n",
    "\n",
    "# 1. Instantiate the model structure\n",
    "classical_model_instance = Transformer(\n",
    "    num_tokens=VOCAB_SIZE,\n",
    "    max_seq_len=MAX_SEQ_LEN,\n",
    "    task='mlm',\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_heads=NUM_HEADS,\n",
    "    num_transformer_blocks=NUM_BLOCKS,\n",
    "    mlp_hidden_size=MLP_HIDDEN_SIZE\n",
    ")\n",
    "\n",
    "# 2. Call your load_model function with the correct arguments\n",
    "classical_params, classical_tokenizer = load_model(\n",
    "    model_path=CLASSICAL_MODEL_PATH,\n",
    "    model_instance=classical_model_instance,\n",
    "    init_batch=init_batch\n",
    ")\n",
    "print(f\"Classical tokenizer vocabulary size: {len(classical_tokenizer.vocab)}\\n\")\n",
    "\n",
    "# --- Load Quantum Model ---\n",
    "print(\"Loading quantum model...\")\n",
    "\n",
    "# 1. Instantiate the model structure\n",
    "quantum_model_instance = Transformer(\n",
    "    num_tokens=VOCAB_SIZE,\n",
    "    max_seq_len=MAX_SEQ_LEN,\n",
    "    task='mlm',\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_heads=NUM_HEADS,\n",
    "    num_transformer_blocks=NUM_BLOCKS,\n",
    "    mlp_hidden_size=MLP_HIDDEN_SIZE,\n",
    "    quantum_attn_circuit=get_circuit(),\n",
    "    quantum_mlp_circuit=get_circuit()\n",
    ")\n",
    "\n",
    "# 2. Call your load_model function\n",
    "quantum_params, quantum_tokenizer = load_model(\n",
    "    model_path=QUANTUM_MODEL_PATH,\n",
    "    model_instance=quantum_model_instance,\n",
    "    init_batch=init_batch\n",
    ")\n",
    "print(f\"Quantum tokenizer vocabulary size: {len(quantum_tokenizer.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "406892e8-1532-4996-85a4-282b57fa2942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. PREDICTION FUNCTION (CORRECTED) ---\n",
    "\n",
    "from functools import partial # <-- ADD THIS IMPORT\n",
    "\n",
    "# JIT-compile the prediction step for speed\n",
    "# We tell JIT that the 2nd argument (arg 1) is a 'static' function\n",
    "@partial(jax.jit, static_argnums=(1,))\n",
    "def predict(params, model_apply_fn, inputs):\n",
    "    logits = model_apply_fn({'params': params}, inputs, train=False)\n",
    "    return logits\n",
    "\n",
    "def predict_masked_input(text, model_instance, params, tokenizer, top_k=5):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors=\"jax\", padding=\"max_length\", max_length=MAX_SEQ_LEN)\n",
    "    input_ids = inputs.input_ids\n",
    "    \n",
    "    # Find the position of the [MASK] token\n",
    "    mask_token_id = tokenizer.mask_token_id\n",
    "    mask_position = jnp.where(input_ids == mask_token_id, 1, 0).argmax(axis=-1)[0]\n",
    "    \n",
    "    if mask_position == 0:\n",
    "        print(f\"Warning: Could not find [MASK] token in '{text}'\")\n",
    "        return\n",
    "    \n",
    "    # Get model predictions\n",
    "    # We pass the model's .apply function as the static argument\n",
    "    logits = predict(params, model_instance.apply, input_ids)\n",
    "    \n",
    "    # Get the logits for the [MASK] token's position\n",
    "    mask_logits = logits[0, mask_position, :]\n",
    "    \n",
    "    # Find the top K predicted token IDs\n",
    "    top_k_indices = jnp.argsort(mask_logits)[-top_k:][::-1]\n",
    "    top_k_tokens = tokenizer.convert_ids_to_tokens(top_k_indices)\n",
    "    top_k_scores = jax.nn.softmax(mask_logits)[top_k_indices]\n",
    "    \n",
    "    print(f\"Input: '{text}'\")\n",
    "    print(\"Predictions:\")\n",
    "    for token, score in zip(top_k_tokens, top_k_scores):\n",
    "        print(f\"  - {token} (Score: {score:.4f})\")\n",
    "\n",
    "def evaluate_on_list(texts, model_instance, params, tokenizer, top_k=5):\n",
    "    for text in texts:\n",
    "        predict_masked_input(text, model_instance, params, tokenizer, top_k)\n",
    "        print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "129d72a7-bb2f-4e15-919d-f5123d5a8e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TensorFlow and JAX classes are deprecated and will be removed in Transformers v5. We recommend migrating to PyTorch classes or pinning your version of Transformers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "--- Classical Model Batch Prediction ---\n",
      "Input: 'One day, a [MASK] girl named Lily found a needle in her room'\n",
      "Predictions:\n",
      "  - s (Score: 0.0361)\n",
      "  - ##e (Score: 0.0312)\n",
      "  - t (Score: 0.0292)\n",
      "  - . (Score: 0.0270)\n",
      "  - ##a (Score: 0.0265)\n",
      "---\n",
      "Input: 'Once upon a [MASK], there was a little car named Beep.'\n",
      "Predictions:\n",
      "  - ##e (Score: 0.0324)\n",
      "  - s (Score: 0.0322)\n",
      "  - t (Score: 0.0274)\n",
      "  - ##o (Score: 0.0258)\n",
      "  - ##he (Score: 0.0256)\n",
      "---\n",
      "Input: 'One day, a little fish named Fin [MASK] swimming near the shore.'\n",
      "Predictions:\n",
      "  - s (Score: 0.0331)\n",
      "  - ##e (Score: 0.0326)\n",
      "  - t (Score: 0.0276)\n",
      "  - ##a (Score: 0.0268)\n",
      "  - ##he (Score: 0.0255)\n",
      "---\n",
      "Input: 'Once upon a time, in a small yard, there was a small daisy. The daisy had a name. [MASK] name was Daisy. Daisy was very small, but she was also very happy.'\n",
      "Predictions:\n",
      "  - t (Score: 0.0485)\n",
      "  - a (Score: 0.0405)\n",
      "  - ##a (Score: 0.0368)\n",
      "  - . (Score: 0.0365)\n",
      "  - ##e (Score: 0.0354)\n",
      "---\n",
      "Input: 'Tom kicked the ball high in the sky. The [MASK] went far, far away.'\n",
      "Predictions:\n",
      "  - ##he (Score: 0.0332)\n",
      "  - ##o (Score: 0.0308)\n",
      "  - ##e (Score: 0.0307)\n",
      "  - t (Score: 0.0300)\n",
      "  - s (Score: 0.0296)\n",
      "---\n",
      "\n",
      "==============================\n",
      "--- Quantum Model Batch Prediction ---\n",
      "Input: 'One day, a [MASK] girl named Lily found a needle in her room'\n",
      "Predictions:\n",
      "  - t (Score: 0.0545)\n",
      "  - a (Score: 0.0429)\n",
      "  - ##he (Score: 0.0383)\n",
      "  - ##a (Score: 0.0377)\n",
      "  - ##e (Score: 0.0370)\n",
      "---\n",
      "Input: 'Once upon a [MASK], there was a little car named Beep.'\n",
      "Predictions:\n",
      "  - t (Score: 0.0551)\n",
      "  - a (Score: 0.0439)\n",
      "  - ##he (Score: 0.0398)\n",
      "  - ##a (Score: 0.0376)\n",
      "  - ##e (Score: 0.0373)\n",
      "---\n",
      "Input: 'One day, a little fish named Fin [MASK] swimming near the shore.'\n",
      "Predictions:\n",
      "  - t (Score: 0.0544)\n",
      "  - a (Score: 0.0432)\n",
      "  - ##he (Score: 0.0387)\n",
      "  - ##a (Score: 0.0378)\n",
      "  - ##e (Score: 0.0364)\n",
      "---\n",
      "Input: 'Once upon a time, in a small yard, there was a small daisy. The daisy had a name. [MASK] name was Daisy. Daisy was very small, but she was also very happy.'\n",
      "Predictions:\n",
      "  - t (Score: 0.0523)\n",
      "  - a (Score: 0.0402)\n",
      "  - ##a (Score: 0.0385)\n",
      "  - ##e (Score: 0.0359)\n",
      "  - ##he (Score: 0.0338)\n",
      "---\n",
      "Input: 'Tom kicked the ball high in the sky. The [MASK] went far, far away.'\n",
      "Predictions:\n",
      "  - t (Score: 0.0553)\n",
      "  - a (Score: 0.0438)\n",
      "  - ##he (Score: 0.0399)\n",
      "  - ##e (Score: 0.0378)\n",
      "  - ##a (Score: 0.0375)\n",
      "---\n",
      "\n",
      "--- Inference Complete ---\n"
     ]
    }
   ],
   "source": [
    "# --- 5. RUN INFERENCE ---\n",
    "# We use simple sentences similar to the TinyStories dataset\n",
    "\n",
    "inference_dataset = [\n",
    "    \"One day, a [MASK] girl named Lily found a needle in her room\",\n",
    "    \"Once upon a [MASK], there was a little car named Beep.\",\n",
    "    \"One day, a little fish named Fin [MASK] swimming near the shore.\",\n",
    "    \"Once upon a time, in a small yard, there was a small daisy. The daisy had a name. [MASK] name was Daisy. Daisy was very small, but she was also very happy.\",\n",
    "    \"Tom kicked the ball high in the sky. The [MASK] went far, far away.\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\"--- Classical Model Batch Prediction ---\")\n",
    "evaluate_on_list(\n",
    "    texts=inference_dataset, \n",
    "    model_instance=classical_model_instance, \n",
    "    params=classical_params, \n",
    "    tokenizer=classical_tokenizer,\n",
    "    top_k=5  # Show top 5 predictions\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\"--- Quantum Model Batch Prediction ---\")\n",
    "evaluate_on_list(\n",
    "    texts=inference_dataset, \n",
    "    model_instance=quantum_model_instance, \n",
    "    params=quantum_params, \n",
    "    tokenizer=quantum_tokenizer,\n",
    "    top_k=5  # Show top 5 predictions\n",
    ")\n",
    "\n",
    "print(\"\\n--- Inference Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6424e215-56ce-49ee-88c4-5011a56d8210",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
