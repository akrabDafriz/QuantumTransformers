{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0422aff2-e099-4bd4-b3e2-9efbed8f83c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-09 00:04:29.221818: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1762621469.232576  721966 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1762621469.235955  721966 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1762621469.245500  721966 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1762621469.245511  721966 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1762621469.245512  721966 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1762621469.245513  721966 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-11-09 00:04:29.248384: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Please first ``pip install -U qiskit`` to enable related functionality in translation module\n",
      "Please first ``pip install -U cirq`` to enable related functionality in translation module\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAX devices: [gpu(id=0)]\n"
     ]
    }
   ],
   "source": [
    "# --- 1. FRAMEWORK SETUP (MUST BE FIRST) ---\n",
    "import tensorflow as tf\n",
    "# Ensure TF does not see GPU and grab all GPU memory.\n",
    "tf.config.set_visible_devices([], device_type='GPU')\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import os\n",
    "import pickle\n",
    "from flax import serialization\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "import numpy as np\n",
    "\n",
    "# Import custom modules\n",
    "from quantum_transformers.transformers import Transformer\n",
    "from quantum_transformers.quantum_layer import get_circuit\n",
    "\n",
    "print(\"JAX devices:\", jax.devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ea22fca-0aec-4efd-869a-2d588800a25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path):\n",
    "    \"\"\"Loads a trained model state and its tokenizer.\"\"\"\n",
    "    print(f\"Loading model from: {model_path}\")\n",
    "    \n",
    "    # 1. Load the tokenizer\n",
    "    tokenizer_file = os.path.join(model_path, 'tinystories_tokenizer_directory', 'tokenizer.json')\n",
    "    if not os.path.exists(tokenizer_file):\n",
    "        # Fallback for old save structure\n",
    "        tokenizer_file = os.path.join(model_path, 'tokenizer.json')\n",
    "        if not os.path.exists(tokenizer_file):\n",
    "            raise FileNotFoundError(f\"Could not find tokenizer.json in {model_path} or its subdirectories.\")\n",
    "            \n",
    "    tokenizer = PreTrainedTokenizerFast(tokenizer_file=tokenizer_file)\n",
    "    \n",
    "    # 2. Determine if it's a quantum model by checking path name\n",
    "    is_quantum = 'quantum' in model_path.lower()\n",
    "    \n",
    "    # 3. Instantiate the correct model architecture\n",
    "    # --- THIS MUST MATCH THE NEW mlm_training_2.py ---\n",
    "    \n",
    "    vocab_size = tokenizer.vocab_size\n",
    "    mlp_size = 8 \n",
    "    # --- NEW: Set parameters to match new training script ---\n",
    "    num_blocks = 8\n",
    "    vqc_shape = (4,)\n",
    "    \n",
    "    print(f\"Instantiating model: Quantum={is_quantum}, Vocab={vocab_size}, MLP_Size={mlp_size}, Blocks={num_blocks}\")\n",
    "\n",
    "    if is_quantum:\n",
    "        print(f\"Using VQC Shape: {vqc_shape}\")\n",
    "        model_instance = Transformer(\n",
    "            num_tokens=vocab_size,\n",
    "            max_seq_len=128,\n",
    "            task='mlm',\n",
    "            hidden_size=8,\n",
    "            num_heads=2,\n",
    "            num_transformer_blocks=num_blocks, # <-- Updated\n",
    "            mlp_hidden_size=mlp_size, \n",
    "            dropout=0.0,\n",
    "            quantum_w_shape=vqc_shape, # <-- Updated\n",
    "            quantum_attn_circuit=get_circuit(),\n",
    "            quantum_mlp_circuit=get_circuit()\n",
    "        )\n",
    "    else:\n",
    "        model_instance = Transformer(\n",
    "            num_tokens=vocab_size,\n",
    "            max_seq_len=128,\n",
    "            task='mlm',\n",
    "            hidden_size=8,\n",
    "            num_heads=2,\n",
    "            num_transformer_blocks=num_blocks, # <-- Updated\n",
    "            mlp_hidden_size=mlp_size, \n",
    "            dropout=0.0\n",
    "        )\n",
    "    \n",
    "    # 4. Load the trained parameters\n",
    "    params_file = os.path.join(model_path, 'model_params.msgpack')\n",
    "    with open(params_file, 'rb') as f:\n",
    "        params_bytes = f.read()\n",
    "    params = serialization.from_bytes(target=None, encoded_bytes=params_bytes)['params']\n",
    "    \n",
    "    print(\"Model and tokenizer loaded successfully.\")\n",
    "    return model_instance, params, tokenizer\n",
    "\n",
    "def predict_masked_batch(texts, model_instance, params, tokenizer, top_k=5):\n",
    "    \"\"\"Tokenizes a batch of texts, predicts the [MASK] token, and decodes.\"\"\"\n",
    "    \n",
    "    # Tokenize the batch\n",
    "    inputs = tokenizer(texts, return_tensors='jax', padding=True, truncation=True, max_length=128)\n",
    "    input_ids = inputs['input_ids']\n",
    "    \n",
    "    # Get model predictions (logits)\n",
    "    logits = model_instance.apply({'params': params}, input_ids, train=False)\n",
    "    \n",
    "    # Find the [MASK] token's position in each text\n",
    "    mask_token_id = tokenizer.mask_token_id\n",
    "    mask_indices = jnp.where(input_ids == mask_token_id)\n",
    "    \n",
    "    results = []\n",
    "    for i in range(len(texts)):\n",
    "        # Find the row index (batch item) and column index (sequence position)\n",
    "        # for the mask token in this specific text\n",
    "        \n",
    "        # --- FIX for multiple masks in one batch ---\n",
    "        # Find the first mask token corresponding to this batch item 'i'\n",
    "        mask_for_this_item = jnp.where(mask_indices[0] == i)[0]\n",
    "        if len(mask_for_this_item) == 0:\n",
    "            print(f\"Warning: No [MASK] token found in text: {texts[i]}\")\n",
    "            continue\n",
    "            \n",
    "        seq_idx = mask_indices[1][mask_for_this_item[0]]\n",
    "        batch_idx = i\n",
    "        # --- END FIX ---\n",
    "        \n",
    "        # Get the logits for just that one token\n",
    "        mask_logits = logits[batch_idx, seq_idx, :]\n",
    "        \n",
    "        # Apply softmax to get probabilities\n",
    "        probs = jax.nn.softmax(mask_logits)\n",
    "        \n",
    "        # Get the top_k predictions\n",
    "        top_probs, top_indices = jax.lax.top_k(probs, k=top_k)\n",
    "        \n",
    "        # Decode the tokens\n",
    "        predicted_tokens = tokenizer.convert_ids_to_tokens(top_indices)\n",
    "        results.append({\n",
    "            \"text\": texts[i],\n",
    "            \"predictions\": list(zip(predicted_tokens, np.array(top_probs)))\n",
    "        })\n",
    "        \n",
    "    return results\n",
    "\n",
    "def evaluate_on_list(texts, model_instance, params, tokenizer, top_k=5):\n",
    "    \"\"\"Helper to run prediction and print nicely.\"\"\"\n",
    "    results = predict_masked_batch(texts, model_instance, params, tokenizer, top_k)\n",
    "    for item in results:\n",
    "        print(f\"Input: '{item['text']}'\")\n",
    "        print(\"Predictions:\")\n",
    "        for token, score in item['predictions']:\n",
    "            print(f\"  - {token} (Score: {score:.4f})\")\n",
    "        print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92f13698-2c9a-4bd4-bd57-513a47975a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: ../../models/mlm_classical\n",
      "Instantiating model: Quantum=False, Vocab=1000, MLP_Size=8, Blocks=8\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'params'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m QUANTUM_MODEL_PATH = \u001b[33m'\u001b[39m\u001b[33m../../models/mlm_quantum\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# --- Load both models ---\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m classical_model_instance, classical_params, classical_tokenizer = \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCLASSICAL_MODEL_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m quantum_model_instance, quantum_params, quantum_tokenizer = load_model(QUANTUM_MODEL_PATH)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 60\u001b[39m, in \u001b[36mload_model\u001b[39m\u001b[34m(model_path)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(params_file, \u001b[33m'\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     59\u001b[39m     params_bytes = f.read()\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m params = \u001b[43mserialization\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoded_bytes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams_bytes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mparams\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mModel and tokenizer loaded successfully.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model_instance, params, tokenizer\n",
      "\u001b[31mKeyError\u001b[39m: 'params'"
     ]
    }
   ],
   "source": [
    "# --- Paths from your training script ---\n",
    "CLASSICAL_MODEL_PATH = '../../models/mlm_classical'\n",
    "QUANTUM_MODEL_PATH = '../../models/mlm_quantum'\n",
    "\n",
    "# --- Load both models ---\n",
    "classical_model_instance, classical_params, classical_tokenizer = load_model(CLASSICAL_MODEL_PATH)\n",
    "quantum_model_instance, quantum_params, quantum_tokenizer = load_model(QUANTUM_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406892e8-1532-4996-85a4-282b57fa2942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Dataset for Inference ---\n",
    "inference_dataset = [\n",
    "    \"One day, a [MASK] girl named Lily found a needle in her room\",\n",
    "    \"Once upon a [MASK], there was a little car named Beep.\",\n",
    "    \"One day, a little fish named Fin [MASK] swimming near the shore.\",\n",
    "    \"Once upon a time, in a small yard, there was a small daisy. The daisy had a name. [MASK] name was Daisy. Daisy was very small, but she was also very happy.\",\n",
    "    \"Tom kicked the ball high in the sky. The [MASK] went far, far away.\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\"--- Classical Model Batch Prediction ---\")\n",
    "evaluate_on_list(\n",
    "    texts=inference_dataset, \n",
    "    model_instance=classical_model_instance, \n",
    "    params=classical_params, \n",
    "    tokenizer=classical_tokenizer,\n",
    "    top_k=5  # Show top 5 predictions\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\"--- Quantum Model Batch Prediction ---\")\n",
    "evaluate_on_list(\n",
    "    texts=inference_dataset, \n",
    "    model_instance=quantum_model_instance, \n",
    "    params=quantum_params, \n",
    "    tokenizer=quantum_tokenizer,\n",
    "    top_k=5  # Show top 5 predictions\n",
    ")\n",
    "\n",
    "print(\"\\n--- Inference Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129d72a7-bb2f-4e15-919d-f5123d5a8e56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6424e215-56ce-49ee-88c4-5011a56d8210",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
