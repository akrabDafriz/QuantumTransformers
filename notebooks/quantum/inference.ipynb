{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0422aff2-e099-4bd4-b3e2-9efbed8f83c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 16:23:24.367585: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1762334604.378235 1624529 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1762334604.381849 1624529 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1762334604.391548 1624529 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1762334604.391557 1624529 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1762334604.391558 1624529 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1762334604.391559 1624529 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-11-05 16:23:24.394354: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Please first ``pip install -U qiskit`` to enable related functionality in translation module\n",
      "Please first ``pip install -U cirq`` to enable related functionality in translation module\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available JAX devices:\n",
      "- gpu:0 (NVIDIA GeForce RTX 4090)\n"
     ]
    }
   ],
   "source": [
    "# --- 1. FRAMEWORK SETUP (MUST BE FIRST) ---\n",
    "import tensorflow as tf\n",
    "# Ensure TF does not see GPU and grab all GPU memory.\n",
    "# This MUST run before JAX is imported.\n",
    "tf.config.set_visible_devices([], device_type='GPU')\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import os\n",
    "from flax.training import train_state\n",
    "from transformers import AutoTokenizer\n",
    "import pickle\n",
    "\n",
    "# --- END FRAMEWORK SETUP ---\n",
    "\n",
    "# Import custom modules\n",
    "# We need to add the project root to the path if running from 'notebooks/quantum'\n",
    "import sys\n",
    "sys.path.append('../..') \n",
    "\n",
    "# --- CORRECTED IMPORT: This is the function we will use ---\n",
    "from quantum_transformers.inference import load_model \n",
    "from quantum_transformers.transformers import Transformer\n",
    "from quantum_transformers.quantum_layer import get_circuit\n",
    "\n",
    "print(\"Available JAX devices:\")\n",
    "for d in jax.devices():\n",
    "    print(f\"- {d} ({d.device_kind})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ea22fca-0aec-4efd-869a-2d588800a25f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classical model path: /dafriz/QuantumTransformers/models/mlm_classical\n",
      "Quantum model path: /dafriz/QuantumTransformers/models/mlm_quantum\n"
     ]
    }
   ],
   "source": [
    "# --- 2. DEFINE PATHS ---\n",
    "# These paths must match the ones used in mlm_training.py\n",
    "\n",
    "CLASSICAL_MODEL_PATH = '../../models/mlm_classical'\n",
    "QUANTUM_MODEL_PATH = '../../models/mlm_quantum'\n",
    "\n",
    "print(f\"Classical model path: {os.path.abspath(CLASSICAL_MODEL_PATH)}\")\n",
    "print(f\"Quantum model path: {os.path.abspath(QUANTUM_MODEL_PATH)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92f13698-2c9a-4bd4-bd57-513a47975a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading classical model...\n",
      "Model and tokenizer loaded from ../../models/mlm_classical\n",
      "Classical tokenizer vocabulary size: 1000\n",
      "\n",
      "Loading quantum model...\n",
      "Model and tokenizer loaded from ../../models/mlm_quantum\n",
      "Quantum tokenizer vocabulary size: 1000\n"
     ]
    }
   ],
   "source": [
    "# --- 3. LOAD MODELS AND TOKENIZERS (CORRECTED) ---\n",
    "\n",
    "# Define model hyperparameters (must match training)\n",
    "VOCAB_SIZE = 1000\n",
    "MAX_SEQ_LEN = 128\n",
    "HIDDEN_SIZE = 8\n",
    "NUM_HEADS = 2\n",
    "NUM_BLOCKS = 4\n",
    "MLP_HIDDEN_SIZE = 8\n",
    "\n",
    "# Create a dummy batch to initialize the model state (required by load_model)\n",
    "# Shape is (batch_size, max_seq_len)\n",
    "init_batch = jnp.ones((1, MAX_SEQ_LEN), dtype=jnp.int32)\n",
    "\n",
    "# --- Load Classical Model ---\n",
    "print(\"Loading classical model...\")\n",
    "\n",
    "# 1. Instantiate the model structure\n",
    "classical_model_instance = Transformer(\n",
    "    num_tokens=VOCAB_SIZE,\n",
    "    max_seq_len=MAX_SEQ_LEN,\n",
    "    task='mlm',\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_heads=NUM_HEADS,\n",
    "    num_transformer_blocks=NUM_BLOCKS,\n",
    "    mlp_hidden_size=MLP_HIDDEN_SIZE\n",
    ")\n",
    "\n",
    "# 2. Call your load_model function with the correct arguments\n",
    "classical_params, classical_tokenizer = load_model(\n",
    "    model_path=CLASSICAL_MODEL_PATH,\n",
    "    model_instance=classical_model_instance,\n",
    "    init_batch=init_batch\n",
    ")\n",
    "print(f\"Classical tokenizer vocabulary size: {len(classical_tokenizer.vocab)}\\n\")\n",
    "\n",
    "# --- Load Quantum Model ---\n",
    "print(\"Loading quantum model...\")\n",
    "\n",
    "# 1. Instantiate the model structure\n",
    "quantum_model_instance = Transformer(\n",
    "    num_tokens=VOCAB_SIZE,\n",
    "    max_seq_len=MAX_SEQ_LEN,\n",
    "    task='mlm',\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_heads=NUM_HEADS,\n",
    "    num_transformer_blocks=NUM_BLOCKS,\n",
    "    mlp_hidden_size=MLP_HIDDEN_SIZE,\n",
    "    quantum_attn_circuit=get_circuit(),\n",
    "    quantum_mlp_circuit=get_circuit()\n",
    ")\n",
    "\n",
    "# 2. Call your load_model function\n",
    "quantum_params, quantum_tokenizer = load_model(\n",
    "    model_path=QUANTUM_MODEL_PATH,\n",
    "    model_instance=quantum_model_instance,\n",
    "    init_batch=init_batch\n",
    ")\n",
    "print(f\"Quantum tokenizer vocabulary size: {len(quantum_tokenizer.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "406892e8-1532-4996-85a4-282b57fa2942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. PREDICTION FUNCTION (CORRECTED) ---\n",
    "\n",
    "from functools import partial # <-- ADD THIS IMPORT\n",
    "\n",
    "# JIT-compile the prediction step for speed\n",
    "# We tell JIT that the 2nd argument (arg 1) is a 'static' function\n",
    "@partial(jax.jit, static_argnums=(1,))\n",
    "def predict(params, model_apply_fn, inputs):\n",
    "    logits = model_apply_fn({'params': params}, inputs, train=False)\n",
    "    return logits\n",
    "\n",
    "def predict_masked_input(text, model_instance, params, tokenizer, top_k=5):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors=\"jax\", padding=\"max_length\", max_length=MAX_SEQ_LEN)\n",
    "    input_ids = inputs.input_ids\n",
    "    \n",
    "    # Find the position of the [MASK] token\n",
    "    mask_token_id = tokenizer.mask_token_id\n",
    "    mask_position = jnp.where(input_ids == mask_token_id, 1, 0).argmax(axis=-1)[0]\n",
    "    \n",
    "    if mask_position == 0:\n",
    "        print(f\"Warning: Could not find [MASK] token in '{text}'\")\n",
    "        return\n",
    "    \n",
    "    # Get model predictions\n",
    "    # We pass the model's .apply function as the static argument\n",
    "    logits = predict(params, model_instance.apply, input_ids)\n",
    "    \n",
    "    # Get the logits for the [MASK] token's position\n",
    "    mask_logits = logits[0, mask_position, :]\n",
    "    \n",
    "    # Find the top K predicted token IDs\n",
    "    top_k_indices = jnp.argsort(mask_logits)[-top_k:][::-1]\n",
    "    top_k_tokens = tokenizer.convert_ids_to_tokens(top_k_indices)\n",
    "    top_k_scores = jax.nn.softmax(mask_logits)[top_k_indices]\n",
    "    \n",
    "    print(f\"Input: '{text}'\")\n",
    "    print(\"Predictions:\")\n",
    "    for token, score in zip(top_k_tokens, top_k_scores):\n",
    "        print(f\"  - {token} (Score: {score:.4f})\")\n",
    "\n",
    "def evaluate_on_list(texts, model_instance, params, tokenizer, top_k=5):\n",
    "    for text in texts:\n",
    "        predict_masked_input(text, model_instance, params, tokenizer, top_k)\n",
    "        print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "129d72a7-bb2f-4e15-919d-f5123d5a8e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TensorFlow and JAX classes are deprecated and will be removed in Transformers v5. We recommend migrating to PyTorch classes or pinning your version of Transformers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "--- Classical Model Batch Prediction ---\n",
      "Input: 'The little cat sat on the [MASK].'\n",
      "Predictions:\n",
      "  - ##fast (Score: 0.0210)\n",
      "  - ##hou (Score: 0.0171)\n",
      "  - ##that  (Score: 0.0109)\n",
      "  - 6 (Score: 0.0097)\n",
      "  - J (Score: 0.0085)\n",
      "---\n",
      "Input: 'Tom has a [MASK] ball.'\n",
      "Predictions:\n",
      "  - ##hou (Score: 0.0197)\n",
      "  - ##fast (Score: 0.0155)\n",
      "  - ##!\"\n",
      "\n",
      " (Score: 0.0090)\n",
      "  - ##ou (Score: 0.0084)\n",
      "  - ##that  (Score: 0.0081)\n",
      "---\n",
      "Input: 'Lily saw a big [MASK] flower.'\n",
      "Predictions:\n",
      "  - ##hou (Score: 0.0197)\n",
      "  - ##fast (Score: 0.0153)\n",
      "  - ##!\"\n",
      "\n",
      " (Score: 0.0090)\n",
      "  - ##ou (Score: 0.0084)\n",
      "  - ##that  (Score: 0.0081)\n",
      "---\n",
      "Input: 'The sun is very [MASK].'\n",
      "Predictions:\n",
      "  - ##hou (Score: 0.0186)\n",
      "  - ##fast (Score: 0.0155)\n",
      "  - ##!\"\n",
      "\n",
      " (Score: 0.0086)\n",
      "  - ##that  (Score: 0.0085)\n",
      "  - J (Score: 0.0083)\n",
      "---\n",
      "Input: 'A dog ran in the [MASK].'\n",
      "Predictions:\n",
      "  - ##hou (Score: 0.0203)\n",
      "  - ##fast (Score: 0.0137)\n",
      "  - ##!\"\n",
      "\n",
      " (Score: 0.0095)\n",
      "  - ##ou (Score: 0.0076)\n",
      "  - ##that  (Score: 0.0074)\n",
      "---\n",
      "\n",
      "==============================\n",
      "--- Quantum Model Batch Prediction ---\n",
      "Input: 'The little cat sat on the [MASK].'\n",
      "Predictions:\n",
      "  - ##upon a  (Score: 0.0184)\n",
      "  - ##run (Score: 0.0101)\n",
      "  - ##liked to  (Score: 0.0101)\n",
      "  - ##Ã‚ (Score: 0.0099)\n",
      "  - ##special  (Score: 0.0085)\n",
      "---\n",
      "Input: 'Tom has a [MASK] ball.'\n",
      "Predictions:\n",
      "  - ##upon a  (Score: 0.0254)\n",
      "  - ##lea (Score: 0.0084)\n",
      "  - ##jo (Score: 0.0082)\n",
      "  - ##bb (Score: 0.0079)\n",
      "  - ##special  (Score: 0.0065)\n",
      "---\n",
      "Input: 'Lily saw a big [MASK] flower.'\n",
      "Predictions:\n",
      "  - ##upon a  (Score: 0.0253)\n",
      "  - ##lea (Score: 0.0084)\n",
      "  - ##jo (Score: 0.0082)\n",
      "  - ##bb (Score: 0.0079)\n",
      "  - ##special  (Score: 0.0065)\n",
      "---\n",
      "Input: 'The sun is very [MASK].'\n",
      "Predictions:\n",
      "  - ##liked to  (Score: 0.0185)\n",
      "  - ##s and  (Score: 0.0152)\n",
      "  - ##upon a  (Score: 0.0104)\n",
      "  - ##_ (Score: 0.0101)\n",
      "  - ##. (Score: 0.0085)\n",
      "---\n",
      "Input: 'A dog ran in the [MASK].'\n",
      "Predictions:\n",
      "  - ##took (Score: 0.0112)\n",
      "  - ##im (Score: 0.0104)\n",
      "  - ##liked to  (Score: 0.0080)\n",
      "  - ##st  (Score: 0.0077)\n",
      "  - ##upon a  (Score: 0.0071)\n",
      "---\n",
      "\n",
      "--- Inference Complete ---\n"
     ]
    }
   ],
   "source": [
    "# --- 5. RUN INFERENCE ---\n",
    "# We use simple sentences similar to the TinyStories dataset\n",
    "\n",
    "inference_dataset = [\n",
    "    \"The little cat sat on the [MASK].\",\n",
    "    \"Tom has a [MASK] ball.\",\n",
    "    \"Lily saw a big [MASK] flower.\",\n",
    "    \"The sun is very [MASK].\",\n",
    "    \"A dog ran in the [MASK].\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\"--- Classical Model Batch Prediction ---\")\n",
    "evaluate_on_list(\n",
    "    texts=inference_dataset, \n",
    "    model_instance=classical_model_instance, \n",
    "    params=classical_params, \n",
    "    tokenizer=classical_tokenizer,\n",
    "    top_k=5  # Show top 5 predictions\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\"--- Quantum Model Batch Prediction ---\")\n",
    "evaluate_on_list(\n",
    "    texts=inference_dataset, \n",
    "    model_instance=quantum_model_instance, \n",
    "    params=quantum_params, \n",
    "    tokenizer=quantum_tokenizer,\n",
    "    top_k=5  # Show top 5 predictions\n",
    ")\n",
    "\n",
    "print(\"\\n--- Inference Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6424e215-56ce-49ee-88c4-5011a56d8210",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
