{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0422aff2-e099-4bd4-b3e2-9efbed8f83c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-01 22:03:25.336455: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1762009405.346565 2038996 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1762009405.349927 2038996 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1762009405.358939 2038996 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1762009405.358946 2038996 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1762009405.358948 2038996 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1762009405.358949 2038996 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "Please first ``pip install -U qiskit`` to enable related functionality in translation module\n",
      "Please first ``pip install -U cirq`` to enable related functionality in translation module\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# Import custom modules\n",
    "from quantum_transformers.datasets import get_mlm_dataloaders\n",
    "from quantum_transformers.transformers import Transformer\n",
    "from quantum_transformers.quantum_layer import get_circuit\n",
    "from quantum_transformers.inference import load_model, evaluate_on_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ea22fca-0aec-4efd-869a-2d588800a25f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up environment for inference...\n",
      "Both models found, in ../../models/mlm_classical and ../../models/mlm_quantum\n"
     ]
    }
   ],
   "source": [
    "# --- 1. SETUP ---\n",
    "print(\"Setting up environment for inference...\")\n",
    "\n",
    "# Ensure TF does not see GPU and grab all GPU memory.\n",
    "tf.config.set_visible_devices([], device_type='GPU')\n",
    "\n",
    "# Define directories\n",
    "CLASSICAL_MODEL_PATH = '../../models/mlm_classical'\n",
    "QUANTUM_MODEL_PATH = '../../models/mlm_quantum'\n",
    "\n",
    "# Check if model paths exist\n",
    "if not os.path.exists(CLASSICAL_MODEL_PATH) or not os.path.exists(QUANTUM_MODEL_PATH):\n",
    "    print(f\"Error: Model directories not found.\")\n",
    "    print(f\"Please run 'mlm_training.py' first to train and save the models.\")\n",
    "    exit()\n",
    "else:\n",
    "    print(f\"Both models found, in {CLASSICAL_MODEL_PATH} and {QUANTUM_MODEL_PATH}\")\n",
    "\n",
    "# Define model hyperparameters (MUST match training script)\n",
    "block_size = 128\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92f13698-2c9a-4bd4-bd57-513a47975a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading tokenizer and initialization batch...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c0cb27ed87446fea631327768da2a9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/75710 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (522 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2548c83f92c4098a67dc0d29e966652",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8413 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0c37aef880b475a82ab6ac78c504f56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9347 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc1fecb26e1145a09977e5c664965c49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/75710 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72d8b216b5bb4acbbf6b665c99fe35f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8413 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce039d623a164d4889e58ecbe4f407bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9347 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer and init batch loaded successfully.\n",
      "Tokenizer vocabulary size: 30522\n"
     ]
    }
   ],
   "source": [
    "# --- 2. GET INITIALIZATION DATA ---\n",
    "# We MUST load the dataloader to get two things:\n",
    "# 1. The exact 'tokenizer' used during training.\n",
    "# 2. An 'init_batch_input' to create the model \"scaffold\".\n",
    "print(\"\\nLoading tokenizer and initialization batch...\")\n",
    "try:\n",
    "    (train_dataloader_gen, _, _), tokenizer = get_mlm_dataloaders(\n",
    "        dataset_name='Helsinki-NLP/opus_books',\n",
    "        model_checkpoint='bert-base-uncased',\n",
    "        block_size=block_size,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    init_batch_tuple = next(iter(train_dataloader_gen()))\n",
    "    init_batch_input = init_batch_tuple[0]\n",
    "    print(f\"Tokenizer and init batch loaded successfully.\")\n",
    "    print(f\"Tokenizer vocabulary size: {len(tokenizer.vocab)}\")\n",
    "except StopIteration:\n",
    "    print(\"ERROR: Dataloader is empty. Cannot get initialization batch.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "406892e8-1532-4996-85a4-282b57fa2942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Instantiating model structures...\n"
     ]
    }
   ],
   "source": [
    "# --- 3. DEFINE MODEL STRUCTURES ---\n",
    "# We need to create instances of the models so 'load_model' knows\n",
    "# what structure to load the saved weights into.\n",
    "print(\"\\nInstantiating model structures...\")\n",
    "\n",
    "classical_model = Transformer(\n",
    "    num_tokens=len(tokenizer.vocab),\n",
    "    max_seq_len=block_size,\n",
    "    task='mlm',\n",
    "    hidden_size=8,\n",
    "    num_heads=2,\n",
    "    num_transformer_blocks=4,\n",
    "    mlp_hidden_size=4,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "quantum_model = Transformer(\n",
    "    num_tokens=len(tokenizer.vocab),\n",
    "    max_seq_len=block_size,\n",
    "    task='mlm',\n",
    "    hidden_size=8,\n",
    "    num_heads=2,\n",
    "    num_transformer_blocks=4,\n",
    "    mlp_hidden_size=4,\n",
    "    dropout=0.1,\n",
    "    quantum_attn_circuit=get_circuit(),\n",
    "    quantum_mlp_circuit=get_circuit()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "129d72a7-bb2f-4e15-919d-f5123d5a8e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "--- 4. Loading Saved Models ---\n",
      "==============================\n",
      "\n",
      "--- Loading Classical Model for Inference ---\n",
      "Model and tokenizer loaded from ../../models/mlm_classical\n",
      "\n",
      "--- Loading Quantum Model for Inference ---\n",
      "Model and tokenizer loaded from ../../models/mlm_quantum\n"
     ]
    }
   ],
   "source": [
    "# --- 4. LOAD MODELS ---\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\"--- 4. Loading Saved Models ---\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "print(\"\\n--- Loading Classical Model for Inference ---\")\n",
    "classical_params, classical_tokenizer = load_model(\n",
    "    model_path=CLASSICAL_MODEL_PATH,\n",
    "    model_instance=classical_model,\n",
    "    init_batch=init_batch_input\n",
    ")\n",
    "\n",
    "print(\"\\n--- Loading Quantum Model for Inference ---\")\n",
    "quantum_params, quantum_tokenizer = load_model(\n",
    "    model_path=QUANTUM_MODEL_PATH,\n",
    "    model_instance=quantum_model,\n",
    "    init_batch=init_batch_input\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "278d1ad7-a3dd-4e8a-8c79-f45071e05873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "--- 5. Running Inference ---\n",
      "==============================\n",
      "\n",
      "==============================\n",
      "--- Classical Model Batch Prediction ---\n",
      "--- Running batch inference on 5 sentences ---\n",
      "\n",
      "Example 1:\n",
      "Input: 'He went to the [MASK] to buy some bread.'\n",
      "Top predictions:\n",
      "  - ,               (Logit: 6.99)\n",
      "  - the             (Logit: 6.51)\n",
      "  - and             (Logit: 6.07)\n",
      "\n",
      "Example 2:\n",
      "Input: 'The capital of France is [MASK].'\n",
      "Top predictions:\n",
      "  - ,               (Logit: 7.01)\n",
      "  - the             (Logit: 6.45)\n",
      "  - .               (Logit: 6.18)\n",
      "\n",
      "Example 3:\n",
      "Input: 'She put the book on the [MASK].'\n",
      "Top predictions:\n",
      "  - ,               (Logit: 6.85)\n",
      "  - the             (Logit: 6.62)\n",
      "  - and             (Logit: 6.07)\n",
      "\n",
      "Example 4:\n",
      "Input: 'Let's go for a [MASK] in the park.'\n",
      "Top predictions:\n",
      "  - ,               (Logit: 6.85)\n",
      "  - the             (Logit: 6.62)\n",
      "  - and             (Logit: 6.07)\n",
      "\n",
      "Example 5:\n",
      "Input: 'The [MASK] is barking at the mailman.'\n",
      "Top predictions:\n",
      "  - ,               (Logit: 6.88)\n",
      "  - the             (Logit: 6.51)\n",
      "  - .               (Logit: 6.23)\n",
      "\n",
      "--- Batch inference complete ---\n",
      "\n",
      "==============================\n",
      "--- Quantum Model Batch Prediction ---\n",
      "--- Running batch inference on 5 sentences ---\n",
      "\n",
      "Example 1:\n",
      "Input: 'He went to the [MASK] to buy some bread.'\n",
      "Top predictions:\n",
      "  - ,               (Logit: 7.06)\n",
      "  - the             (Logit: 6.57)\n",
      "  - .               (Logit: 6.30)\n",
      "\n",
      "Example 2:\n",
      "Input: 'The capital of France is [MASK].'\n",
      "Top predictions:\n",
      "  - ,               (Logit: 7.18)\n",
      "  - the             (Logit: 6.49)\n",
      "  - .               (Logit: 6.25)\n",
      "\n",
      "Example 3:\n",
      "Input: 'She put the book on the [MASK].'\n",
      "Top predictions:\n",
      "  - ,               (Logit: 6.99)\n",
      "  - the             (Logit: 6.62)\n",
      "  - .               (Logit: 6.27)\n",
      "\n",
      "Example 4:\n",
      "Input: 'Let's go for a [MASK] in the park.'\n",
      "Top predictions:\n",
      "  - ,               (Logit: 6.99)\n",
      "  - the             (Logit: 6.62)\n",
      "  - .               (Logit: 6.27)\n",
      "\n",
      "Example 5:\n",
      "Input: 'The [MASK] is barking at the mailman.'\n",
      "Top predictions:\n",
      "  - ,               (Logit: 7.02)\n",
      "  - the             (Logit: 6.55)\n",
      "  - .               (Logit: 6.25)\n",
      "\n",
      "--- Batch inference complete ---\n",
      "\n",
      "--- Inference Complete ---\n"
     ]
    }
   ],
   "source": [
    "# --- 5. RUN INFERENCE ---\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\"--- 5. Running Inference ---\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "# Create your test dataset\n",
    "inference_dataset = [\n",
    "    \"He went to the [MASK] to buy some bread.\",\n",
    "    \"The capital of France is [MASK].\",\n",
    "    \"She put the book on the [MASK].\",\n",
    "    \"Let's go for a [MASK] in the park.\",\n",
    "    \"The [MASK] is barking at the mailman.\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\"--- Classical Model Batch Prediction ---\")\n",
    "evaluate_on_list(\n",
    "    texts=inference_dataset, \n",
    "    model=classical_model, \n",
    "    params=classical_params, \n",
    "    tokenizer=classical_tokenizer,\n",
    "    top_k=3  # Show top 3 predictions\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\"--- Quantum Model Batch Prediction ---\")\n",
    "evaluate_on_list(\n",
    "    texts=inference_dataset, \n",
    "    model=quantum_model, \n",
    "    params=quantum_params, \n",
    "    tokenizer=quantum_tokenizer,\n",
    "    top_k=3  # Show top 3 predictions\n",
    ")\n",
    "\n",
    "print(\"\\n--- Inference Complete ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6424e215-56ce-49ee-88c4-5011a56d8210",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
