{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook pre-trains a Transformer model on the Helsinki-NLP/opus_books dataset using the Masked Language Modeling (MLM) objective. We will train both a classical and a quantum-hybrid version of the model to compare their performance. The key evaluation metric for this task is Perplexity (PPL), where a lower value is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T22:23:18.422798Z",
     "iopub.status.busy": "2023-10-09T22:23:18.422674Z",
     "iopub.status.idle": "2023-10-09T22:23:29.124548Z",
     "shell.execute_reply": "2023-10-09T22:23:29.124133Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-15 14:34:06.128381: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-09-15 14:34:06.128519: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-09-15 14:34:06.165393: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-09-15 14:34:49.496595: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Please first ``pip install -U qiskit`` to enable related functionality in translation module\n",
      "Please first ``pip install -U cirq`` to enable related functionality in translation module\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import tensorflow as tf\n",
    "\n",
    "# Ensure TF does not see GPU and grab all GPU memory.\n",
    "tf.config.set_visible_devices([], device_type='GPU')  \n",
    "\n",
    "# Import our custom modules\n",
    "from quantum_transformers.datasets import get_mlm_dataloaders\n",
    "from quantum_transformers.training import train_and_evaluate\n",
    "from quantum_transformers.transformers import Transformer\n",
    "from quantum_transformers.quantum_layer import get_circuit\n",
    "\n",
    "# Define the directory where datasets are stored/cached\n",
    "data_dir = './data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models are trained using the following devices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T22:23:29.126883Z",
     "iopub.status.busy": "2023-10-09T22:23:29.126510Z",
     "iopub.status.idle": "2023-10-09T22:23:29.334752Z",
     "shell.execute_reply": "2023-10-09T22:23:29.334351Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available JAX devices:\n",
      "- gpu:0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU)\n"
     ]
    }
   ],
   "source": [
    "print(\"Available JAX devices:\")\n",
    "for d in jax.devices():\n",
    "    print(f\"- {d} ({d.device_kind})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how big is the vocabulary, and see an example of one example review (both in tokenized and raw form)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T22:23:29.336517Z",
     "iopub.status.busy": "2023-10-09T22:23:29.336376Z",
     "iopub.status.idle": "2023-10-09T22:25:08.033572Z",
     "shell.execute_reply": "2023-10-09T22:25:08.033125Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing the dataset for MLM...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09d4f3214ce24bc6bab2daac07b17fed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/75710 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (874 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c795ea8d3785421d82e70f91ae3ef194",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8413 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94408a12b1a249d9b7871eb5ead54dc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9347 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dec53b850e0485d97127acbe39f0a63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/75710 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1761c866a2e4d66b692ff3b81b92ea5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8413 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8cecfb7cd8746dfbf82176a72b91121",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9347 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset loading complete.\n",
      "Tokenizer vocabulary size: 30522\n"
     ]
    }
   ],
   "source": [
    "# Set data loading parameters\n",
    "block_size = 128  # The size of our text chunks\n",
    "batch_size = 16   # How many chunks to process at once\n",
    "\n",
    "print(\"Loading and preparing the dataset for MLM...\")\n",
    "\n",
    "# Get the dataloaders and the tokenizer\n",
    "(train_dataloader, val_dataloader, test_dataloader), tokenizer = get_mlm_dataloaders(\n",
    "    dataset_name='Helsinki-NLP/opus_books',\n",
    "    model_checkpoint='bert-base-uncased',\n",
    "    # data_dir=data_dir,\n",
    "    block_size=block_size,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "print(\"\\nDataset loading complete.\")\n",
    "print(f\"Tokenizer vocabulary size: {len(tokenizer.vocab)}\")\n",
    "\n",
    "## fourth cell. This is a crucial code cell that calls our new dataloader function. The first time you run this, it will download the Helsinki-NLP/opus_books dataset and process it, which might take a few minutes depending on your internet connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T22:25:08.035283Z",
     "iopub.status.busy": "2023-10-09T22:25:08.035136Z",
     "iopub.status.idle": "2023-10-09T23:00:53.539551Z",
     "shell.execute_reply": "2023-10-09T23:00:53.539071Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Classical Transformer Training ---\n",
      "Number of parameters = 521498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 1191it [00:54, 21.83it/s, Loss=10.2358, PPL=27883.92]\n",
      "                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 10.5655, Val Loss = 10.2883, Val PPL = 29386.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2: 1191it [01:05, 18.23it/s, Loss=10.0146, PPL=22351.32]\n",
      "                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss = 10.1303, Val Loss = 9.9069, Val PPL = 20068.01\n",
      "Total training time = 126.44s, best validation loss = 9.9069 at epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 146it [00:03, 36.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss = 9.9067, Test PPL = 20064.53\n",
      "\n",
      "--- Classical Transformer Training Finished ---\n",
      "Final Test Perplexity: 20064.5273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Train a Classical Transformer as a Baseline ---\n",
    "\n",
    "print(\"--- Starting Classical Transformer Training ---\")\n",
    "\n",
    "# Define the classical model's hyperparameters\n",
    "classical_model = Transformer(\n",
    "    num_tokens=len(tokenizer.vocab),\n",
    "    max_seq_len=block_size,\n",
    "    task='mlm',\n",
    "    hidden_size=8,              # Small hidden size for comparability with quantum models\n",
    "    num_heads=2,\n",
    "    num_transformer_blocks=4,\n",
    "    mlp_hidden_size=4,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "# Train and evaluate the classical model\n",
    "# Unpack the returned tuple into two variables\n",
    "classical_test_loss, classical_test_ppl = train_and_evaluate(\n",
    "    model=classical_model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloader=val_dataloader,\n",
    "    test_dataloader=test_dataloader,\n",
    "    task='mlm',\n",
    "    num_epochs=2 \n",
    ")\n",
    "\n",
    "print(\"\\n--- Classical Transformer Training Finished ---\")\n",
    "# Use the new variable to print the result\n",
    "print(f\"Final Test Perplexity: {classical_test_ppl:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Quantum Transformer Training ---\n",
      "Number of parameters = 520490\n",
      "Number of parameters = 520490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 1191it [07:25,  2.68it/s, Loss=10.3625, PPL=31648.71]\n",
      "Epoch 1/2: 1191it [07:25,  2.68it/s, Loss=10.3625, PPL=31648.71]\n",
      "                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 10.5900, Val Loss = 10.3707, Val PPL = 31911.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2: 1191it [04:59,  3.97it/s, Loss=9.9607, PPL=21178.38] \n",
      "Epoch 2/2: 1191it [04:59,  3.97it/s, Loss=9.9607, PPL=21178.38] \n",
      "                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss = 10.2565, Val Loss = 10.0239, Val PPL = 22558.38\n",
      "Total training time = 796.59s, best validation loss = 10.0239 at epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 146it [00:28,  5.19it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss = 10.0137, Test PPL = 22329.61\n",
      "\n",
      "--- Quantum Transformer Training Finished ---\n",
      "Final Test Perplexity: 22329.6133\n"
     ]
    }
   ],
   "source": [
    "# --- Train the Quantum Transformer ---\n",
    "\n",
    "print(\"--- Starting Quantum Transformer Training ---\")\n",
    "\n",
    "# Define the quantum model's hyperparameters (identical to classical for fair comparison)\n",
    "quantum_model = Transformer(\n",
    "    num_tokens=len(tokenizer.vocab),\n",
    "    max_seq_len=block_size,\n",
    "    task='mlm',\n",
    "    hidden_size=8,\n",
    "    num_heads=2,\n",
    "    num_transformer_blocks=4,\n",
    "    mlp_hidden_size=4,\n",
    "    dropout=0.1,\n",
    "    quantum_attn_circuit=get_circuit(),  # Activate the quantum attention\n",
    "    quantum_mlp_circuit=get_circuit()    # Activate the quantum MLP\n",
    ")\n",
    "\n",
    "# Train and evaluate the quantum model\n",
    "# Unpack the returned tuple into two variables\n",
    "quantum_test_loss, quantum_test_ppl = train_and_evaluate(\n",
    "    model=quantum_model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloader=val_dataloader,\n",
    "    test_dataloader=test_dataloader,\n",
    "    task='mlm',\n",
    "    num_epochs=2  # A shorter run for demonstration; increase for better results\n",
    ")\n",
    "\n",
    "print(\"\\n--- Quantum Transformer Training Finished ---\")\n",
    "print(f\"Final Test Perplexity: {quantum_test_ppl:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add this the next time you work on this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # After classical model definition, add:\n",
    "# print(\"\\nClassical Model:\")\n",
    "# params = classical_model.init(jax.random.PRNGKey(0), jnp.ones((1, block_size)), train=False)['params']\n",
    "# num_params = sum(p.size for p in jax.tree_util.tree_leaves(params))\n",
    "# print(f\"Number of parameters: {num_params:,}\")\n",
    "\n",
    "# # And after quantum model definition, add:\n",
    "# print(\"\\nQuantum Model:\")\n",
    "# params = quantum_model.init(jax.random.PRNGKey(0), jnp.ones((1, block_size)), train=False)['params']\n",
    "# num_params = sum(p.size for p in jax.tree_util.tree_leaves(params))\n",
    "# print(f\"Number of parameters: {num_params:,}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
